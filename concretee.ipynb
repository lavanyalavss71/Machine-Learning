{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Linear Regression on Concrete Dataset\n",
    "---\n",
    "In concrete dataset we are finding linear relationship between input vraiables and target variables..\n",
    "Here below we are importing pytorch to convert into matrix form for processing data\n",
    "And importing pandas for reading the csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selecting 500 rows for training the data\n",
    "- Splitting the dataset into two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./Concrete.csv\")[:500]\n",
    "data1= dataset[['cement','slag','flyash','water','superplasticizer','coarseaggregate','fineaggregate','age']]\n",
    "data2 = dataset['csMPa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't directly convert the data into tensor arrays .So we are firstly,\n",
    "converting the dataset into Numpy array.Then we convert into tensor arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1=np.array(data1)\n",
    "dataset2=np.array(data2).reshape(-1,1)\n",
    "feature=torch.tensor(dataset1,dtype=torch.float32)\n",
    "labels=torch.tensor(dataset2,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the linear model using the function **Linear** from **torch.nn** class.\n",
    "- (8,1) indicates number of input variables i.e, features and number of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0506, -0.1082, -0.3471, -0.3114,  0.1355,  0.1758, -0.2127, -0.1315]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2808], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(8,1)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -41.7151],\n",
      "        [ -39.0776],\n",
      "        [-100.9044],\n",
      "        [-113.3993],\n",
      "        [-134.7393],\n",
      "        [ -86.9453],\n",
      "        [-110.6638],\n",
      "        [ -66.3396],\n",
      "        [ -78.7907],\n",
      "        [ -60.8686],\n",
      "        [ -99.2273],\n",
      "        [ -91.0727],\n",
      "        [ -95.4334],\n",
      "        [ -91.3221],\n",
      "        [ -76.6023],\n",
      "        [ -80.3801],\n",
      "        [ -88.5543],\n",
      "        [-118.7382],\n",
      "        [ -74.4942],\n",
      "        [ -80.8605],\n",
      "        [ -83.5960],\n",
      "        [ -80.3997],\n",
      "        [ -77.1116],\n",
      "        [-100.3917],\n",
      "        [-116.5498],\n",
      "        [-104.0548],\n",
      "        [ -98.1689],\n",
      "        [ -94.4059],\n",
      "        [ -63.6041],\n",
      "        [ -58.1065],\n",
      "        [-120.9266],\n",
      "        [-123.1150],\n",
      "        [-111.0647],\n",
      "        [ -92.6979],\n",
      "        [-127.4917],\n",
      "        [-106.3753],\n",
      "        [ -74.5460],\n",
      "        [ -77.2297],\n",
      "        [ -69.0232],\n",
      "        [ -94.5380],\n",
      "        [ -82.5685],\n",
      "        [-107.9283],\n",
      "        [-118.8703],\n",
      "        [ -92.2175],\n",
      "        [ -71.7587],\n",
      "        [ -60.8420],\n",
      "        [ -65.0524],\n",
      "        [ -86.3315],\n",
      "        [ -71.7840],\n",
      "        [ -63.5775],\n",
      "        [ -89.0670],\n",
      "        [-103.1594],\n",
      "        [ -82.7007],\n",
      "        [ -84.7569],\n",
      "        [ -77.6377],\n",
      "        [ -88.3107],\n",
      "        [-105.1929],\n",
      "        [ -87.7846],\n",
      "        [ -96.5942],\n",
      "        [ -69.0751],\n",
      "        [-108.4316],\n",
      "        [-110.6200],\n",
      "        [ -85.7365],\n",
      "        [-114.9968],\n",
      "        [ -98.7826],\n",
      "        [-106.2432],\n",
      "        [-124.0663],\n",
      "        [ -66.3130],\n",
      "        [ -83.1675],\n",
      "        [ -46.5934],\n",
      "        [ -89.1783],\n",
      "        [ -43.8331],\n",
      "        [-117.4976],\n",
      "        [ -84.0701],\n",
      "        [-126.7410],\n",
      "        [-108.5458],\n",
      "        [-103.9755],\n",
      "        [-117.4976],\n",
      "        [-124.8364],\n",
      "        [-107.5504],\n",
      "        [-117.4976],\n",
      "        [-123.1719],\n",
      "        [ -89.7660],\n",
      "        [ -83.2956],\n",
      "        [ -77.4929],\n",
      "        [ -10.6450],\n",
      "        [ -83.2956],\n",
      "        [ -74.1416],\n",
      "        [ -83.2956],\n",
      "        [ -92.8284],\n",
      "        [ -77.3507],\n",
      "        [ -83.2956],\n",
      "        [ -85.4445],\n",
      "        [ -89.7044],\n",
      "        [ -44.3592],\n",
      "        [-118.0237],\n",
      "        [ -84.5962],\n",
      "        [-127.2671],\n",
      "        [-109.0719],\n",
      "        [-104.5016],\n",
      "        [-118.0237],\n",
      "        [-125.3625],\n",
      "        [-108.0765],\n",
      "        [-118.0237],\n",
      "        [-123.6980],\n",
      "        [ -90.2922],\n",
      "        [ -83.8217],\n",
      "        [ -78.0190],\n",
      "        [ -11.1711],\n",
      "        [ -83.8217],\n",
      "        [ -74.6678],\n",
      "        [ -83.8217],\n",
      "        [ -93.3545],\n",
      "        [ -77.8768],\n",
      "        [ -83.8217],\n",
      "        [ -85.9706],\n",
      "        [ -92.4665],\n",
      "        [ -47.1212],\n",
      "        [-120.7857],\n",
      "        [ -87.3582],\n",
      "        [-130.0291],\n",
      "        [-111.8339],\n",
      "        [-107.2637],\n",
      "        [-120.7857],\n",
      "        [-128.1245],\n",
      "        [-110.8386],\n",
      "        [-120.7857],\n",
      "        [-126.4601],\n",
      "        [ -93.0542],\n",
      "        [ -86.5837],\n",
      "        [ -80.7811],\n",
      "        [ -13.9331],\n",
      "        [ -86.5837],\n",
      "        [ -77.4298],\n",
      "        [ -86.5837],\n",
      "        [ -96.1165],\n",
      "        [ -80.6389],\n",
      "        [ -86.5837],\n",
      "        [ -88.7327],\n",
      "        [ -96.1492],\n",
      "        [ -50.8040],\n",
      "        [-124.4684],\n",
      "        [ -91.0410],\n",
      "        [-133.7119],\n",
      "        [-115.5166],\n",
      "        [-110.9464],\n",
      "        [-124.4684],\n",
      "        [-131.8073],\n",
      "        [-114.5213],\n",
      "        [-124.4684],\n",
      "        [-130.1428],\n",
      "        [ -96.7369],\n",
      "        [ -90.2664],\n",
      "        [ -84.4638],\n",
      "        [ -17.6158],\n",
      "        [ -90.2664],\n",
      "        [ -81.1125],\n",
      "        [ -90.2664],\n",
      "        [ -99.7993],\n",
      "        [ -84.3216],\n",
      "        [ -90.2664],\n",
      "        [ -92.4154],\n",
      "        [-100.7526],\n",
      "        [ -55.4074],\n",
      "        [-129.0719],\n",
      "        [ -95.6444],\n",
      "        [-138.3153],\n",
      "        [-120.1201],\n",
      "        [-115.5498],\n",
      "        [-129.0719],\n",
      "        [-136.4107],\n",
      "        [-119.1247],\n",
      "        [-129.0719],\n",
      "        [-134.7462],\n",
      "        [-101.3403],\n",
      "        [ -94.8699],\n",
      "        [ -22.2193],\n",
      "        [ -94.8699],\n",
      "        [ -85.7159],\n",
      "        [ -94.8699],\n",
      "        [-104.4027],\n",
      "        [ -88.9250],\n",
      "        [ -94.8699],\n",
      "        [ -97.0188],\n",
      "        [-118.2935],\n",
      "        [-119.7403],\n",
      "        [-121.5816],\n",
      "        [-125.2644],\n",
      "        [-131.0515],\n",
      "        [-120.4906],\n",
      "        [-121.9374],\n",
      "        [-123.7788],\n",
      "        [-127.4615],\n",
      "        [-133.2486],\n",
      "        [-111.0855],\n",
      "        [-112.5322],\n",
      "        [-114.3736],\n",
      "        [-118.0563],\n",
      "        [-123.8435],\n",
      "        [ -82.0247],\n",
      "        [ -83.4715],\n",
      "        [ -85.3128],\n",
      "        [ -88.9956],\n",
      "        [ -94.7827],\n",
      "        [ -88.1843],\n",
      "        [ -89.6310],\n",
      "        [ -91.4724],\n",
      "        [ -95.1551],\n",
      "        [-100.9423],\n",
      "        [ -93.3828],\n",
      "        [ -94.8295],\n",
      "        [ -96.6709],\n",
      "        [-100.3536],\n",
      "        [-106.1408],\n",
      "        [ -81.6244],\n",
      "        [ -83.0712],\n",
      "        [ -84.9126],\n",
      "        [ -88.5953],\n",
      "        [ -94.3825],\n",
      "        [ -99.3047],\n",
      "        [-100.7515],\n",
      "        [-102.5928],\n",
      "        [-106.2756],\n",
      "        [-112.0627],\n",
      "        [ -86.9159],\n",
      "        [ -88.3627],\n",
      "        [ -90.2041],\n",
      "        [ -93.8868],\n",
      "        [ -99.6739],\n",
      "        [ -65.3033],\n",
      "        [ -66.7501],\n",
      "        [ -68.5915],\n",
      "        [ -72.2742],\n",
      "        [ -78.0613],\n",
      "        [ -65.3216],\n",
      "        [ -66.7684],\n",
      "        [ -68.6097],\n",
      "        [ -72.2925],\n",
      "        [ -78.0796],\n",
      "        [ -93.0521],\n",
      "        [ -94.4989],\n",
      "        [ -96.3402],\n",
      "        [-100.0230],\n",
      "        [-105.8101],\n",
      "        [-115.1061],\n",
      "        [-116.5529],\n",
      "        [-118.3942],\n",
      "        [-122.0770],\n",
      "        [-127.8641],\n",
      "        [-118.4737],\n",
      "        [-119.9205],\n",
      "        [-121.7618],\n",
      "        [-125.4445],\n",
      "        [-131.2317],\n",
      "        [-109.0915],\n",
      "        [-110.5383],\n",
      "        [-112.3797],\n",
      "        [-116.0624],\n",
      "        [-121.8496],\n",
      "        [-109.6432],\n",
      "        [-111.0900],\n",
      "        [-112.9313],\n",
      "        [-116.6141],\n",
      "        [-122.4012],\n",
      "        [ -81.7805],\n",
      "        [ -83.2272],\n",
      "        [ -85.0686],\n",
      "        [ -88.7513],\n",
      "        [ -94.5385],\n",
      "        [ -87.1244],\n",
      "        [ -88.5712],\n",
      "        [ -90.4126],\n",
      "        [ -94.0953],\n",
      "        [ -99.8824],\n",
      "        [ -92.1074],\n",
      "        [ -93.5542],\n",
      "        [ -95.3956],\n",
      "        [ -99.0783],\n",
      "        [-104.8655],\n",
      "        [ -92.0261],\n",
      "        [ -93.4729],\n",
      "        [ -95.3143],\n",
      "        [ -98.9970],\n",
      "        [-104.7841],\n",
      "        [ -98.8326],\n",
      "        [-100.2794],\n",
      "        [-102.1207],\n",
      "        [-105.8035],\n",
      "        [-111.5906],\n",
      "        [ -88.1874],\n",
      "        [ -89.6342],\n",
      "        [ -91.4756],\n",
      "        [ -95.1583],\n",
      "        [-100.9454],\n",
      "        [ -83.4741],\n",
      "        [ -84.9209],\n",
      "        [ -86.7623],\n",
      "        [ -90.4450],\n",
      "        [ -96.2321],\n",
      "        [-114.2047],\n",
      "        [-115.6515],\n",
      "        [-117.4929],\n",
      "        [-121.1756],\n",
      "        [-126.9627],\n",
      "        [-111.3089],\n",
      "        [-112.7557],\n",
      "        [-114.5971],\n",
      "        [-118.2798],\n",
      "        [-124.0669],\n",
      "        [-115.2302],\n",
      "        [-116.6769],\n",
      "        [-118.5183],\n",
      "        [-122.2010],\n",
      "        [-127.9882],\n",
      "        [-105.8027],\n",
      "        [-107.2495],\n",
      "        [-109.0908],\n",
      "        [-112.7736],\n",
      "        [-118.5607],\n",
      "        [-109.8700],\n",
      "        [-111.3168],\n",
      "        [-113.1582],\n",
      "        [-116.8409],\n",
      "        [-122.6281],\n",
      "        [-106.1679],\n",
      "        [-107.6147],\n",
      "        [-109.4561],\n",
      "        [-113.1388],\n",
      "        [-118.9259],\n",
      "        [ -78.2386],\n",
      "        [ -79.6854],\n",
      "        [ -81.5268],\n",
      "        [ -85.2095],\n",
      "        [ -90.9967],\n",
      "        [ -84.5763],\n",
      "        [ -86.0231],\n",
      "        [ -87.8645],\n",
      "        [ -91.5472],\n",
      "        [ -97.3344],\n",
      "        [ -89.4707],\n",
      "        [ -90.9175],\n",
      "        [ -92.7589],\n",
      "        [ -96.4416],\n",
      "        [-102.2287],\n",
      "        [ -98.2498],\n",
      "        [ -99.6966],\n",
      "        [-101.5380],\n",
      "        [-105.2207],\n",
      "        [-111.0078],\n",
      "        [ -97.8201],\n",
      "        [ -99.2669],\n",
      "        [-101.1083],\n",
      "        [-104.7910],\n",
      "        [-110.5782],\n",
      "        [ -61.4655],\n",
      "        [ -62.9123],\n",
      "        [ -64.7536],\n",
      "        [ -68.4364],\n",
      "        [ -74.2235],\n",
      "        [ -81.6901],\n",
      "        [ -83.1368],\n",
      "        [ -84.9782],\n",
      "        [ -88.6609],\n",
      "        [ -94.4481],\n",
      "        [ -93.9203],\n",
      "        [ -95.3671],\n",
      "        [ -97.2085],\n",
      "        [-100.8912],\n",
      "        [-106.6784],\n",
      "        [ -81.4563],\n",
      "        [ -82.9031],\n",
      "        [ -84.7445],\n",
      "        [ -88.4272],\n",
      "        [ -94.2143],\n",
      "        [ -71.6406],\n",
      "        [ -73.0874],\n",
      "        [ -74.9287],\n",
      "        [ -78.6115],\n",
      "        [ -84.3986],\n",
      "        [ -83.2917],\n",
      "        [ -35.1174],\n",
      "        [ -38.2477],\n",
      "        [ -63.3602],\n",
      "        [ -53.3866],\n",
      "        [-108.5491],\n",
      "        [-113.4253],\n",
      "        [ -78.1310],\n",
      "        [-101.7114],\n",
      "        [-121.1159],\n",
      "        [ -23.2654],\n",
      "        [ -26.1448],\n",
      "        [ -93.9502],\n",
      "        [ -91.8150],\n",
      "        [ -98.4023],\n",
      "        [ -29.2466],\n",
      "        [ -33.3430],\n",
      "        [-108.5356],\n",
      "        [-161.7843],\n",
      "        [-165.5346],\n",
      "        [ -79.2958],\n",
      "        [-135.6284],\n",
      "        [ -33.7976],\n",
      "        [ -52.4856],\n",
      "        [ -36.0432],\n",
      "        [-144.2143],\n",
      "        [ -32.1504],\n",
      "        [-124.0761],\n",
      "        [-103.5563],\n",
      "        [-101.5521],\n",
      "        [-102.6916],\n",
      "        [-123.5958],\n",
      "        [-116.7883],\n",
      "        [-111.3315],\n",
      "        [-108.5187],\n",
      "        [ -83.9012],\n",
      "        [-120.4269],\n",
      "        [-100.0167],\n",
      "        [-113.1229],\n",
      "        [ -90.0817],\n",
      "        [-125.5229],\n",
      "        [-105.0030],\n",
      "        [-102.9989],\n",
      "        [-104.1384],\n",
      "        [-125.0426],\n",
      "        [-118.2350],\n",
      "        [-112.7783],\n",
      "        [-109.9655],\n",
      "        [ -85.3480],\n",
      "        [-121.8737],\n",
      "        [-101.4635],\n",
      "        [-114.5697],\n",
      "        [ -91.5285],\n",
      "        [-127.3643],\n",
      "        [-106.8444],\n",
      "        [-104.8403],\n",
      "        [-105.9798],\n",
      "        [-126.8839],\n",
      "        [-120.0764],\n",
      "        [-114.6197],\n",
      "        [-111.8068],\n",
      "        [ -87.1893],\n",
      "        [-123.7150],\n",
      "        [-103.3048],\n",
      "        [-116.4110],\n",
      "        [ -93.3699],\n",
      "        [-131.0470],\n",
      "        [-110.5271],\n",
      "        [-108.5230],\n",
      "        [-109.6625],\n",
      "        [-130.5667],\n",
      "        [-123.7591],\n",
      "        [-118.3024],\n",
      "        [-115.4896],\n",
      "        [ -90.8721],\n",
      "        [-127.3978],\n",
      "        [-106.9875],\n",
      "        [-120.0938],\n",
      "        [ -97.0526],\n",
      "        [-136.8342],\n",
      "        [-116.3143],\n",
      "        [-114.3102],\n",
      "        [-115.4496],\n",
      "        [-136.3538],\n",
      "        [-129.5463],\n",
      "        [-124.0895],\n",
      "        [-121.2767],\n",
      "        [ -96.6592],\n",
      "        [-133.1849],\n",
      "        [-112.7747],\n",
      "        [-125.8809],\n",
      "        [-102.8398],\n",
      "        [ -86.2350],\n",
      "        [ -86.2350],\n",
      "        [ -86.2350],\n",
      "        [ -86.4112],\n",
      "        [ -82.9468],\n",
      "        [ -82.9468],\n",
      "        [ -82.9468],\n",
      "        [ -83.4729],\n",
      "        [ -83.4729],\n",
      "        [ -83.4729],\n",
      "        [ -89.9177],\n",
      "        [ -89.9177],\n",
      "        [ -89.9177],\n",
      "        [ -90.0939],\n",
      "        [-119.4850],\n",
      "        [-119.5392],\n",
      "        [-119.8509],\n",
      "        [-116.1968],\n",
      "        [-116.2510],\n",
      "        [-116.5628],\n",
      "        [-116.7229],\n",
      "        [-116.7771],\n",
      "        [-117.0889],\n",
      "        [-123.1677],\n",
      "        [-123.2219],\n",
      "        [-123.5337],\n",
      "        [-115.6555],\n",
      "        [-115.7640],\n",
      "        [-132.1709]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds = model(feature)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding mean square loss between targets/labels and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "loss  = mse_loss(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss.backward() function finds the gradient \n",
    "- model.weights.grad means the gradient for weights\n",
    "- model.bias.grad means the gradient for bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -82855.3438,  -18524.4219,  -19147.0469,  -48326.0859,   -2532.9023,\n",
      "         -273598.3438, -220962.6875,  -16361.3682]])\n",
      "tensor([-280.3467])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(model.weight.grad)\n",
    "print(model.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.optim will hold the current state and will update the parameters based on the computed gradients.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- optimizers implement a step() method, that updates the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero_grad() function sets the gradients of all optimized values to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19105.5684, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimize = torch.optim.SGD(model.parameters(), lr=0.00000001)\n",
    "\n",
    "optimize.step()\n",
    "optimize.zero_grad()\n",
    "\n",
    "preds = model(feature)\n",
    "loss = mse_loss(preds, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reduce the loss by varying the epochs value and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(347.8519, grad_fn=<MseLossBackward0>)\n",
      "tensor(253.6053, grad_fn=<MseLossBackward0>)\n",
      "tensor(213.9779, grad_fn=<MseLossBackward0>)\n",
      "tensor(189.9368, grad_fn=<MseLossBackward0>)\n",
      "tensor(173.4416, grad_fn=<MseLossBackward0>)\n",
      "tensor(161.7378, grad_fn=<MseLossBackward0>)\n",
      "tensor(153.3505, grad_fn=<MseLossBackward0>)\n",
      "tensor(147.3132, grad_fn=<MseLossBackward0>)\n",
      "tensor(142.9527, grad_fn=<MseLossBackward0>)\n",
      "tensor(139.7927, grad_fn=<MseLossBackward0>)\n",
      "tensor(137.4944, grad_fn=<MseLossBackward0>)\n",
      "tensor(135.8161, grad_fn=<MseLossBackward0>)\n",
      "tensor(134.5854, grad_fn=<MseLossBackward0>)\n",
      "tensor(133.6788, grad_fn=<MseLossBackward0>)\n",
      "tensor(133.0076, grad_fn=<MseLossBackward0>)\n",
      "tensor(132.5081, grad_fn=<MseLossBackward0>)\n",
      "tensor(132.1343, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.8530, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.6400, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.4777, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.3533, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.2574, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.1828, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.1246, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.0788, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.0426, grad_fn=<MseLossBackward0>)\n",
      "tensor(131.0136, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9904, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9716, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9563, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9437, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9333, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9246, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9172, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9109, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9056, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.9009, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8967, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8931, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8898, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8868, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8840, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8815, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8791, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8768, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8747, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8727, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8707, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8688, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8669, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8651, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8633, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8615, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8598, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8581, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8565, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8549, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8532, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8516, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8499, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8483, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8467, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8451, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8435, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8419, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8404, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8388, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8373, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8357, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8342, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8327, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8312, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8297, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8282, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8266, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8251, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8237, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8222, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8207, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8193, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8178, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8164, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8149, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8134, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8120, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8106, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8092, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8078, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8064, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8049, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8035, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8022, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.8007, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.7994, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.7980, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.7966, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.7953, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.7939, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.7926, grad_fn=<MseLossBackward0>)\n",
      "tensor(130.7912, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimize= torch.optim.SGD(model.parameters(), lr=0.0000001)\n",
    "\n",
    "epochs = 100000\n",
    "for epoch in range(epochs):\n",
    "    preds = model(feature)\n",
    "    loss = mse_loss(preds, labels)\n",
    "    \n",
    "    if (epoch+1)%1000 == 0:\n",
    "        print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimize.step()\n",
    "    optimize.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecbe764c402a74c9c85f4c67e8ec6f24dc6b9bc341300af706a517cc170f4d3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
