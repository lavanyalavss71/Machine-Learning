{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression on College Dataset\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In college Dataset we are find linear relationship between input variables and target variables..\n",
    "  - Here below we are importing pytorch to convert into matrix form for processing data.\n",
    "And importing pandas for reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since there are some categories in columns,we use **get_dummies** function to create dummy variables ie 0 or 1 for categories..\n",
    "* we are using Standard Scalar for good estimation..Standard scalar function normalizes the data by setting mean value as 0 and variance as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   parent_age                1000 non-null   int64  \n",
      " 1   parent_salary             1000 non-null   int64  \n",
      " 2   house_area                1000 non-null   float64\n",
      " 3   average_grades            1000 non-null   float64\n",
      " 4   parent_was_in_college     1000 non-null   bool   \n",
      " 5   type_school_Academic      1000 non-null   uint8  \n",
      " 6   type_school_Vocational    1000 non-null   uint8  \n",
      " 7   school_accreditation_A    1000 non-null   uint8  \n",
      " 8   school_accreditation_B    1000 non-null   uint8  \n",
      " 9   gender_Female             1000 non-null   uint8  \n",
      " 10  gender_Male               1000 non-null   uint8  \n",
      " 11  interest_Interested       1000 non-null   uint8  \n",
      " 12  interest_Less Interested  1000 non-null   uint8  \n",
      " 13  interest_Not Interested   1000 non-null   uint8  \n",
      " 14  interest_Uncertain        1000 non-null   uint8  \n",
      " 15  interest_Very Interested  1000 non-null   uint8  \n",
      " 16  residence_Rural           1000 non-null   uint8  \n",
      " 17  residence_Urban           1000 non-null   uint8  \n",
      "dtypes: bool(1), float64(2), int64(2), uint8(13)\n",
      "memory usage: 52.7 KB\n",
      "None\n",
      "torch.Size([1000, 18])\n",
      "torch.Size([1000, 1]) torch.Size([1000, 18])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./clg_data.csv\").dropna()\n",
    "data = pd.get_dummies(data)\n",
    "features = data.drop([\"will_go_to_college\"],axis =1)\n",
    "# print(features.columns)\n",
    "# features = pd.get_dummies(features)\n",
    "print(features.info())\n",
    "data[\"will_go_to_college\"]=data[\"will_go_to_college\"]/10000\n",
    "labels = data[\"will_go_to_college\"]\n",
    "# labels = pd.get_dummies(labels)\n",
    "npfeatures = np.array(features)\n",
    "features = StandardScaler().fit_transform(features)\n",
    "\n",
    "features = torch.tensor(features, dtype = torch.float32)\n",
    "print(features.shape)\n",
    "nplabels = np.array(labels).reshape(-1,1)\n",
    "labels = torch.tensor(nplabels,dtype = torch.float32)\n",
    "\n",
    "print(labels.shape,features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are performing linear regression in multiple layers in a cascading way to increase the efficiency of the model.So we are using the Sequential method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(18,9),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(9,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "preds=model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding mean square loss between targets/labels and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2279, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "loss  = mse_loss(preds, labels)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.optim will hold the current state and will update the parameters based on the computed gradients.\n",
    "- To construct an Optimizer you have to give it an iterable containing the parameters (all should be Variable s) to optimize. Then, you can specify optimizer-specific options such as the learning rate\n",
    "- optimizers implement a step() method, that updates the parameters.\n",
    "- Zero_grad() function sets the gradients of all optimized values to zero\n",
    "- loss.backward() function finds the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100000\n",
    "for epoch in range(epochs):\n",
    "    preds = model(features)\n",
    "    loss = mse_loss(preds, labels)\n",
    "    \n",
    "    if (epoch+1)%1000 == 0:\n",
    "        print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecbe764c402a74c9c85f4c67e8ec6f24dc6b9bc341300af706a517cc170f4d3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
